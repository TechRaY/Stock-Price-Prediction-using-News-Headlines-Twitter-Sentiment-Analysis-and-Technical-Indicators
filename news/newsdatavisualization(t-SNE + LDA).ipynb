{
  "cells": [
    {
      "metadata": {
        "_uuid": "d433fe6099f6979721464d87698d5416cc405fd7",
        "_cell_guid": "34980676-140e-49d9-872c-e4bf81140dd3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom nltk.stem import PorterStemmer\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndf = pd.read_csv(\"../input/finalsortedmoneycontrol.csv\",error_bad_lines=False,warn_bad_lines=False)\ndf.publish_date = pd.to_datetime(df.date,format=\"%Y/%m/%d\")\ndf.head()\n\nprint(df.publish_date.min())\ns = df.groupby('date').tail(2)\nprint(s.head())\n\nall_headlines = s.News_Headlines.values\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom nltk.corpus import stopwords\nStopWords = stopwords.words(\"english\")\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\n\nsia = SIA()\npos_list = []\nneg_list = []\nneu_list = []\nfor post in all_headlines:\n    post = \" \".join([stemmer.stem(word) for word in str(post).lower().split() if word not in set(StopWords)])\n    res = sia.polarity_scores(post)\n    if res['compound'] > 0.0:\n        pos_list.append(post)\n    elif res['compound'] < 0.0:\n        neg_list.append(post)\n    else:\n        neu_list.append(post)\n        \nprint(\"\\n\")\nprint(\"Number of Positive Headlines : {}\\nNumber of Negative Headlines : {}\\nNumber of Neutral Headlines : {}\".format(len(pos_list),len(neg_list),len(neu_list)))\n\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\n\npos_words = []\nfor line in pos_list:\n    words = tokenizer.tokenize(line)\n    for w in words:\n        pos_words.append(w.lower())\n    \n    neg_words = []\nfor line in neg_list:\n    words = tokenizer.tokenize(line)\n    for w in words:\n        neg_words.append(w.lower())\n        \n## Most common positive words in the headlines\nfrom nltk import FreqDist\npos_words = FreqDist(pos_words)\nfor x in pos_words.most_common(10):\n    print(x[0],\":\",x[1])\n    \n## Most common negative words in the headlines\n\nneg_words = FreqDist(neg_words)\nfor x in neg_words.most_common(10):\n    print(x[0],\":\",x[1])\n    \n## Distribution of words in Positive Headlines\nimport matplotlib\nimport matplotlib.pylab as plt\n%matplotlib inline\nmatplotlib.rcParams['xtick.labelsize'] = 14\nplt.figure(figsize=(20,10))\npos_words.plot(50,cumulative=False)\n\n## Distribution of words in Negative Headlines\n\nplt.figure(figsize=(20,10))\nneg_words.plot(50,cumulative=False)\n\nsample = pos_list+neg_list+neu_list\n\nimport gensim\nfrom gensim import corpora\n\nsample_clean = [text.split() for text in sample] \n\n# Creating the term dictionary of our courpus, where every unique term is assigned an index. \ndictionary = corpora.Dictionary(sample_clean)\n\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in sample_clean]\n\n# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\nnum_topics = 10\n# Running and Trainign LDA model on the document term matrix.\nldamodel = Lda(doc_term_matrix, num_topics=num_topics, id2word = dictionary, passes=50,iterations=100)\n\ndtm = ldamodel.get_document_topics(doc_term_matrix)\nK = ldamodel.num_topics\ntopic_word_matrix = ldamodel.print_topics(K)\n\nprint(\"The topics are: \\n\")\nfor x in topic_word_matrix:\n    print(x[0],\":\",x[1],\"\\n\")\n    \nfrom gensim import matutils\ndocument_topic_matrix = matutils.corpus2dense(corpus=dtm,num_docs=len(all_headlines),num_terms=K)\na = document_topic_matrix.transpose()\n\n\nfrom sklearn.manifold import TSNE\n\n# a t-SNE model\n# angle value close to 1 means sacrificing accuracy for speed\n# pca initializtion usually leads to better results \ntsne_model = TSNE(n_components=2, verbose=1, random_state=0,init='pca',)\n\n# 8-D -> 2-D\ntsne_lda = tsne_model.fit_transform(a)\n\n_lda_keys = []\nfor i in range(a.shape[0]):\n    _lda_keys.append(a[i].argmax())\nlen(_lda_keys)\n\n##### Using Bokeh to plot a interactive-visualization\n\nimport bokeh.plotting as bp\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import show\n\n# 10 colors\ncolormap = np.array([\"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\"#98df8a\", \"#d62728\", \"#ff9896\",\"#bcbd22\", \"#dbdb8d\"])\noutput_notebook()\n\nplot_lda = bp.figure(plot_width=1000, plot_height=1000,\n                     title=\"LDA t-SNE Viz\",\n                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n                     x_axis_type=None, y_axis_type=None, min_border=1)\nn = len(a)\nprint(n)\n\n\ntopic_summaries = [x[1] for x in topic_word_matrix]\ntopic_coord = np.empty((a.shape[1], 2)) * np.nan\nfor topic_num in _lda_keys:\n    topic_coord[topic_num] = tsne_lda[_lda_keys.index(topic_num)]\n    \n# add topic words to graph\nfor i in range(a.shape[1]):\n    plot_lda.text(topic_coord[i, 0], topic_coord[i, 1], [topic_summaries[i]])\n    \nshow(plot_lda)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "finalsortedmoneycontrol.csv\n\n2009-01-01 00:00:00\n        time        date            Source  \\\n0   3.38 pm   2009-01-01  Moneycontrol.com   \n1   9.05 am   2009-01-01     Business Line   \n2   8.44 pm   2009-01-05  Moneycontrol.com   \n3   1.00 pm   2009-01-05  Moneycontrol.com   \n5  12.41 pm   2009-01-06  Moneycontrol.com   \n\n                                      News_Headlines  \n0     infosys Q3 PAT seen at Rs 1572 cr: KRChoksey    \n1           Corporate houses seek CISF protection     \n2    Hold Infosys Tech, TCS, Satyam: Emkay Global     \n3   Infosys' Dec qtr PAT seen at Rs 1491 cr: Angel    \n5  Infosys Q3 PAT seen at Rs 1501.01 cr: Reliance...  \n\n\nNumber of Positive Headlines : 493\nNumber of Negative Headlines : 199\nNumber of Neutral Headlines : 1834\ninfosi : 193\ntop : 95\ninfosys : 85\nrs : 51\ngrowth : 48\nlike : 41\nsay : 40\nstock : 38\ntech : 36\nprofit : 35\ninfosi : 77\ninfosys : 30\nbuy : 18\nrs : 16\navoid : 16\nsay : 14\nco : 13\noverweight : 13\ntech : 12\nbroke : 12\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}